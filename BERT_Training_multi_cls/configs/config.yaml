batch_size: 8
num_workers: 0
pin_memory: False
text_max_length: 256
num_labels: 5
epochs: 30
learning_rate: 3e-5
warmup_ratio: 0.1
validation_ratio: 0.1
device: cuda
log_per_step: 50
model_path: models/bert-base-chinese
data_file: example_dataset.csv
output_dir: output2/bert_checkpoints
LLRD_simple: False
bert_learning_rate: 5e-5
predictor_learning_rate: 1e-4
LLDR_complex: False
base_learning_rate: 2e-5
decay_factor: 0.95 
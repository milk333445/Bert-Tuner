batch_size: 8
num_workers: 0
pin_memory: False
text_max_length: 128
num_labels: 10
epochs: 3
learning_rate: 3e-5
warmup_ratio: 0.1
validation_ratio: 0.1
device: cuda
log_per_step: 50
model_path: models/bert-base-chinese
data_file: tagged_data_clean.csv
output_dir: output/bert_checkpoints
LLRD_simple: False
bert_learning_rate: 5e-5
predictor_learning_rate: 1e-4
LLDR_complex: False
base_learning_rate: 2e-5
decay_factor: 0.95
